{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOpHeFZ1Aw9kMUc298FZ34e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUcNtFpo0vpe","executionInfo":{"status":"ok","timestamp":1743787460627,"user_tz":-330,"elapsed":8683,"user":{"displayName":"Aditya Doke","userId":"16754888037985144975"}},"outputId":"969ab7e7-1d4a-41ac-efc5-b9544cc476ea"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk import pos_tag\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import string\n","import pandas as pd\n","\n","# Download required NLTK data\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk import pos_tag\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import string\n","import pandas as pd\n","\n","# Download all required NLTK data with robust error handling\n","def download_nltk_resources():\n","    resources = {\n","        'punkt': 'tokenizers/punkt',\n","        'stopwords': 'corpora/stopwords',\n","        'averaged_perceptron_tagger': 'taggers/averaged_perceptron_tagger',\n","        'wordnet': 'corpora/wordnet',\n","        'omw-1.4': 'corpora/omw-1.4',\n","        'punkt_tab': 'tokenizers/punkt_tab',\n","        'averaged_perceptron_tagger_eng': 'taggers/averaged_perceptron_tagger_eng'\n","    }\n","\n","    for resource, path in resources.items():\n","        try:\n","            nltk.download(resource)\n","            print(f\"Successfully downloaded {resource}\")\n","        except Exception as e:\n","            print(f\"Failed to download {resource}: {str(e)}\")\n","            # Try alternative download method\n","            try:\n","                nltk.download(resource, download_dir='/root/nltk_data')\n","                nltk.data.path.append('/root/nltk_data')\n","                print(f\"Successfully downloaded {resource} to alternate location\")\n","            except Exception as e2:\n","                print(f\"Completely failed to download {resource}: {str(e2)}\")\n","                if resource == 'averaged_perceptron_tagger_eng':\n","                    # This is a known issue, we can use the regular tagger\n","                    print(\"Will use averaged_perceptron_tagger instead\")\n","\n","download_nltk_resources()\n","\n","# Add nltk data path to system path\n","nltk.data.path.append('/root/nltk_data')\n","\n","# Sample document\n","document = \"\"\"\n","Natural language processing (NLP) is a subfield of linguistics, computer science,\n","and artificial intelligence concerned with the interactions between computers and human language.\n","It focuses on how to program computers to process and analyze large amounts of natural language data.\n","The result is a computer capable of understanding the contents of documents, including the contextual\n","nuances of the language within them. The technology can then accurately extract information and\n","insights contained in the documents as well as categorize and organize the documents themselves.\n","\"\"\"\n","\n","# 1. Tokenization with error handling\n","def tokenize_text(text):\n","    \"\"\"Tokenize text into sentences and words\"\"\"\n","    try:\n","        sentences = sent_tokenize(text)\n","        word_tokens = [word_tokenize(sentence) for sentence in sentences]\n","        return sentences, word_tokens\n","    except LookupError:\n","        # Try to download punkt again if missing\n","        nltk.download('punkt')\n","        nltk.download('punkt_tab')\n","        sentences = sent_tokenize(text)\n","        word_tokens = [word_tokenize(sentence) for sentence in sentences]\n","        return sentences, word_tokens\n","\n","sentences, word_tokens = tokenize_text(document)\n","print(\"\\nSentence Tokens:\")\n","print(sentences)\n","print(\"\\nWord Tokens:\")\n","print(word_tokens)\n","\n","# 2. POS Tagging with fallback\n","def pos_tagging(tokens):\n","    \"\"\"Perform part-of-speech tagging with fallback\"\"\"\n","    try:\n","        return [pos_tag(sentence) for sentence in tokens]\n","    except LookupError:\n","        print(\"POS tagger resource missing, trying to download...\")\n","        nltk.download('averaged_perceptron_tagger')\n","        try:\n","            return [pos_tag(sentence) for sentence in tokens]\n","        except:\n","            print(\"Still can't load POS tagger, using simplified tags\")\n","            # Fallback to universal tagset if full tagger fails\n","            nltk.download('universal_tagset')\n","            return [nltk.pos_tag(sentence, tagset='universal') for sentence in tokens]\n","\n","pos_tags = pos_tagging(word_tokens)\n","print(\"\\nPOS Tags:\")\n","for i, sentence_tags in enumerate(pos_tags):\n","    print(f\"Sentence {i+1}:\")\n","    print(sentence_tags)\n","\n","# 3. Stop Words Removal\n","def remove_stopwords(tokens):\n","    \"\"\"Remove stop words from tokenized sentences\"\"\"\n","    try:\n","        stop_words = set(stopwords.words('english'))\n","        punctuation = set(string.punctuation)\n","        return [\n","            [word.lower() for word in sentence\n","             if word.lower() not in stop_words and word not in punctuation]\n","            for sentence in tokens\n","        ]\n","    except LookupError:\n","        print(\"Stopwords missing, downloading...\")\n","        nltk.download('stopwords')\n","        stop_words = set(stopwords.words('english'))\n","        punctuation = set(string.punctuation)\n","        return [\n","            [word.lower() for word in sentence\n","             if word.lower() not in stop_words and word not in punctuation]\n","            for sentence in tokens\n","        ]\n","\n","filtered_tokens = remove_stopwords(word_tokens)\n","print(\"\\nAfter Stopword Removal:\")\n","print(filtered_tokens)\n","\n","# 4. Stemming\n","def stem_words(tokens):\n","    \"\"\"Apply Porter Stemmer to tokens\"\"\"\n","    porter = PorterStemmer()\n","    return [[porter.stem(word) for word in sentence] for sentence in tokens]\n","\n","stemmed_tokens = stem_words(filtered_tokens)\n","print(\"\\nAfter Stemming:\")\n","print(stemmed_tokens)\n","\n","# 5. Lemmatization\n","def lemmatize_words(tokens):\n","    \"\"\"Apply WordNet Lemmatizer to tokens\"\"\"\n","    try:\n","        lemmatizer = WordNetLemmatizer()\n","        return [[lemmatizer.lemmatize(word) for word in sentence] for sentence in tokens]\n","    except LookupError:\n","        print(\"WordNet missing, downloading...\")\n","        nltk.download('wordnet')\n","        lemmatizer = WordNetLemmatizer()\n","        return [[lemmatizer.lemmatize(word) for word in sentence] for sentence in tokens]\n","\n","lemmatized_tokens = lemmatize_words(filtered_tokens)\n","print(\"\\nAfter Lemmatization:\")\n","print(lemmatized_tokens)\n","\n","# 6. TF-IDF Representation\n","def create_tfidf_representation(sentences):\n","    \"\"\"Create TF-IDF representation of documents\"\"\"\n","    # Reconstruct original sentences from filtered tokens for TF-IDF\n","    clean_sentences = [' '.join(sentence) for sentence in filtered_tokens]\n","\n","    tfidf_vectorizer = TfidfVectorizer(\n","        stop_words='english',\n","        lowercase=True,\n","        tokenizer=word_tokenize,\n","        use_idf=True,\n","        norm='l2',\n","        smooth_idf=True\n","    )\n","\n","    tfidf_matrix = tfidf_vectorizer.fit_transform(clean_sentences)\n","    feature_names = tfidf_vectorizer.get_feature_names_out()\n","\n","    tfidf_df = pd.DataFrame(\n","        tfidf_matrix.toarray(),\n","        columns=feature_names,\n","        index=[f\"Sentence {i+1}\" for i in range(len(clean_sentences))]\n","    )\n","\n","    return tfidf_df, tfidf_vectorizer\n","\n","tfidf_df, vectorizer = create_tfidf_representation(sentences)\n","print(\"\\nTF-IDF Representation:\")\n","print(tfidf_df.head())\n","\n","# Display top terms for each sentence\n","print(\"\\nTop Terms per Sentence:\")\n","for i in range(len(sentences)):\n","    top_terms = tfidf_df.iloc[i].sort_values(ascending=False).head(5)\n","    print(f\"Sentence {i+1}: {', '.join(top_terms.index)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFeTUHEN1J26","executionInfo":{"status":"ok","timestamp":1743787719502,"user_tz":-330,"elapsed":3,"user":{"displayName":"Aditya Doke","userId":"16754888037985144975"}},"outputId":"0652048e-77de-42f8-9dc6-551d7a3907d2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully downloaded punkt\n","Successfully downloaded stopwords\n","Successfully downloaded averaged_perceptron_tagger\n","Successfully downloaded wordnet\n","Successfully downloaded omw-1.4\n","Successfully downloaded punkt_tab\n","Successfully downloaded averaged_perceptron_tagger_eng\n","\n","Sentence Tokens:\n","['\\nNatural language processing (NLP) is a subfield of linguistics, computer science, \\nand artificial intelligence concerned with the interactions between computers and human language.', 'It focuses on how to program computers to process and analyze large amounts of natural language data.', 'The result is a computer capable of understanding the contents of documents, including the contextual \\nnuances of the language within them.', 'The technology can then accurately extract information and \\ninsights contained in the documents as well as categorize and organize the documents themselves.']\n","\n","Word Tokens:\n","[['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', '.'], ['It', 'focuses', 'on', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.'], ['The', 'result', 'is', 'a', 'computer', 'capable', 'of', 'understanding', 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.'], ['The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves', '.']]\n","\n","POS Tags:\n","Sentence 1:\n","[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('linguistics', 'NNS'), (',', ','), ('computer', 'NN'), ('science', 'NN'), (',', ','), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n","Sentence 2:\n","[('It', 'PRP'), ('focuses', 'VBZ'), ('on', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('program', 'NN'), ('computers', 'NNS'), ('to', 'TO'), ('process', 'VB'), ('and', 'CC'), ('analyze', 'VB'), ('large', 'JJ'), ('amounts', 'NNS'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n","Sentence 3:\n","[('The', 'DT'), ('result', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('computer', 'NN'), ('capable', 'NN'), ('of', 'IN'), ('understanding', 'VBG'), ('the', 'DT'), ('contents', 'NNS'), ('of', 'IN'), ('documents', 'NNS'), (',', ','), ('including', 'VBG'), ('the', 'DT'), ('contextual', 'JJ'), ('nuances', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('language', 'NN'), ('within', 'IN'), ('them', 'PRP'), ('.', '.')]\n","Sentence 4:\n","[('The', 'DT'), ('technology', 'NN'), ('can', 'MD'), ('then', 'RB'), ('accurately', 'RB'), ('extract', 'JJ'), ('information', 'NN'), ('and', 'CC'), ('insights', 'NNS'), ('contained', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('documents', 'NNS'), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('categorize', 'NN'), ('and', 'CC'), ('organize', 'VB'), ('the', 'DT'), ('documents', 'NNS'), ('themselves', 'PRP'), ('.', '.')]\n","\n","After Stopword Removal:\n","[['natural', 'language', 'processing', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language'], ['focuses', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data'], ['result', 'computer', 'capable', 'understanding', 'contents', 'documents', 'including', 'contextual', 'nuances', 'language', 'within'], ['technology', 'accurately', 'extract', 'information', 'insights', 'contained', 'documents', 'well', 'categorize', 'organize', 'documents']]\n","\n","After Stemming:\n","[['natur', 'languag', 'process', 'nlp', 'subfield', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag'], ['focus', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data'], ['result', 'comput', 'capabl', 'understand', 'content', 'document', 'includ', 'contextu', 'nuanc', 'languag', 'within'], ['technolog', 'accur', 'extract', 'inform', 'insight', 'contain', 'document', 'well', 'categor', 'organ', 'document']]\n","\n","After Lemmatization:\n","[['natural', 'language', 'processing', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language'], ['focus', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data'], ['result', 'computer', 'capable', 'understanding', 'content', 'document', 'including', 'contextual', 'nuance', 'language', 'within'], ['technology', 'accurately', 'extract', 'information', 'insight', 'contained', 'document', 'well', 'categorize', 'organize', 'document']]\n","\n","TF-IDF Representation:\n","            accurately   amounts   analyze  artificial   capable  categorize  \\\n","Sentence 1    0.000000  0.000000  0.000000    0.272222  0.000000    0.000000   \n","Sentence 2    0.000000  0.339998  0.339998    0.000000  0.000000    0.000000   \n","Sentence 3    0.000000  0.000000  0.000000    0.000000  0.339998    0.000000   \n","Sentence 4    0.308807  0.000000  0.000000    0.000000  0.000000    0.308807   \n","\n","            computer  computers  concerned  contained  ...   nuances  \\\n","Sentence 1  0.214623   0.214623   0.272222   0.000000  ...  0.000000   \n","Sentence 2  0.000000   0.268059   0.000000   0.000000  ...  0.000000   \n","Sentence 3  0.268059   0.000000   0.000000   0.000000  ...  0.339998   \n","Sentence 4  0.000000   0.000000   0.000000   0.308807  ...  0.000000   \n","\n","            organize   process  processing   program    result   science  \\\n","Sentence 1  0.000000  0.000000    0.272222  0.000000  0.000000  0.272222   \n","Sentence 2  0.000000  0.339998    0.000000  0.339998  0.000000  0.000000   \n","Sentence 3  0.000000  0.000000    0.000000  0.000000  0.339998  0.000000   \n","Sentence 4  0.308807  0.000000    0.000000  0.000000  0.000000  0.000000   \n","\n","            subfield  technology  understanding  \n","Sentence 1  0.272222    0.000000       0.000000  \n","Sentence 2  0.000000    0.000000       0.000000  \n","Sentence 3  0.000000    0.000000       0.339998  \n","Sentence 4  0.000000    0.308807       0.000000  \n","\n","[4 rows x 37 columns]\n","\n","Top Terms per Sentence:\n","Sentence 1: language, human, intelligence, concerned, artificial\n","Sentence 2: amounts, analyze, process, focuses, data\n","Sentence 3: capable, nuances, including, contents, contextual\n","Sentence 4: documents, contained, extract, categorize, accurately\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n","/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]}]}]}